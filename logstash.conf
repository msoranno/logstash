input {
  beats {
      port => "5043"
  }
}

filter {
   if [type] == "fst" {
     grok {
       #Si el grok hace match en el mensaje le mete valid en el campo tags
       add_tag => [ "valid" ]
        match => { "message" => "%{DATE:fecha:datestamp}, %{GREEDYDATA:hora} - %{DATA:miembro}:" }
     }
     #si no es valido se descarta
     if "valid" not in [tags] {
       drop { }
     }
   }
   if [type] == "familia" {
     grok {
       #Si el grok hace match en el mensaje le mete valid en el campo tags
       add_tag => [ "valid" ]
        match => { "message" => "%{DATE:fecha:datestamp}, %{GREEDYDATA:hora} - %{DATA:miembro}:" }
     }
     #si no es valido se descarta
     if "valid" not in [tags] {
       drop { }
     }
   }

   if [type] == "mama" {
     grok {
       add_tag => [ "valid" ]
        match => { "message" => "\"Bs%{DATA:disponible}.00\",%{DATE:fecha},\"?.Bs%{DATA:inout}.00\",%{GREEDYDATA:concepto}" }
     }

    date {
          match => [ "fecha", "dd/MM/yyyy" ]
          timezone => "Europe/Madrid"
    }

    mutate {
      gsub => [
          # quitamos las comas
          "inout",      "[,]" , "" ,
          "disponible", "[,]" , ""
      ]
    }

    # convertimos estos 2 campos a integer.
    #Luego en kibana es neceario refrescar los campos por que sino se mantienen string.
    mutate {
        convert => { "inout" => "integer" }
        convert => { "disponible" => "integer" }
    }
   
     if "valid" not in [tags] {
       drop { }
     }

   } #off mama

   if [type] == "auth" {
     grok {
        add_tag => [ "valid" ]
         
        #---
        # mensajes a capturar. en el match cada filtro esta separado por coma:
        # Primer filtro coge los intentos de conexion:
        #    "Jul 13 12:59:23 node-M17x sshd[18182]: Accepted password for elchivo from 54.93.165.2 port 51494 ssh2"
        #    "Jul 13 12:52:27 node-M17x sshd[18099]: Failed password for elchivo from 54.93.165.2 port 51474 ssh2"
        # Segundo filtro coge los comandos ejecutado con su:
        #    "Jul 13 13:00:59 node-M17x sudo:  elchivo : TTY=pts/28 ; PWD=/home/elchivo ; USER=root ; COMMAND=/bin/date"


        match => { "message" => ["%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: %{DATA:[system][auth][ssh][event]} %{DATA:[system][auth][ssh][method]} for (invalid user )?%{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]} port %{NUMBER:[system][auth][ssh][port]} ssh2(: %{GREEDYDATA:[system][auth][ssh][signature]})?" , 
        "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sudo(?:\[%{POSINT:[system][auth][pid]}\])?: \s*%{DATA:[system][auth][user]} :( %{DATA:[system][auth][sudo][error]} ;)? TTY=%{DATA:[system][auth][sudo][tty]} ; PWD=%{DATA:[system][auth][sudo][pwd]} ; USER=%{DATA:[system][auth][sudo][user]} ; COMMAND=%{GREEDYDATA:[system][auth][sudo][command]}"]
        }
     }#--cierra grok

     #------
     # El filtro date se usa para campturar el posible formato de fecha que venga en un campo, y actualizar el campo @timestamp del indice.
     # Esto es bueno hacerlo cuando se van a procesar datos antiguos y quieres que el @timestamp del indice se actualiza con la fecha del fichero.
     # De no hacerlo el campo @timestamp se quedarÃ­a con la fecha de procesamiento y no la real que hay en el log.
     #------
     date {
      match => [ "[system][auth][timestamp]", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
      timezone => "Europe/Madrid"
     }
      
     #---
     # Modulos para saber las coordenadas de las ips
     #---
     geoip {
      source => "[system][auth][ssh][ip]"
      target => "[system][auth][ssh][geoip]"
     }

     #----
     # esto es para descartar mensajes con cierto texto o con cierto tag
     #----

     if ([message] =~ "message repeated") {
        drop { }
     }

     if "valid" not in [tags] {
       drop { }
     }
   }#cierra type auth


}
 

output {
  #---
  # Esto genera salida, es una forma de debuguear
  #---
  stdout {
   codec => rubydebug
  }

  #--- 
  # Al mandar los datos al elastic, se le indica el nombre del indice, si no existe se crea.
  # En este caso el nombre del indice se forma con lo que venga en el campo {type} mas el string _index
  # El campo type se rellena en el filebeat, es una forma de diferenciar los tipos de logs con los que estamos trabajando.
  #---
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    user => "elastic"
    password => "123elastic"
    index => "%{type}_index"
  }

}
